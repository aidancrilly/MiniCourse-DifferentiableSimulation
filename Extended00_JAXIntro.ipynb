{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidancrilly/MiniCourse-DifferentiableSimulation/blob/main/Extended00_JAXIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnRudVHjm429"
      },
      "source": [
        "# Extened introduction to JAX:\n",
        "\n",
        "Extended for I-X Programming Club 2th Dec 2025\n",
        "\n",
        "Resources:\n",
        "\n",
        "- JAX documentation: https://docs.jax.dev/en/latest/quickstart.html\n",
        "\n",
        "\n",
        "# What is JAX?\n",
        "\n",
        "\"JAX is a Python library for accelerator-oriented array computation and program transformation, designed for high-performance numerical computing and large-scale machine learning. It is developed by Google\"\n",
        "\n",
        "JAX uses the Open XLA compiler to compile the same code to run on CPU, GPU and TPU\n",
        "\n",
        "Below are the key libraries to import to use JAX, if you are familiar with regular numpy then jax.numpy will be easy to pick"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTofu7NXm4J9"
      },
      "outputs": [],
      "source": [
        "!pip install optax equinox\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnHe0XuKcfGb"
      },
      "source": [
        "In this introduction, we will go through the key capabilities of JAX, namely:\n",
        "\n",
        "- Just-in-time (JIT) compilation\n",
        "- Vectorising maps\n",
        "- Automatic differeniation\n",
        "\n",
        "These are implemented as JAX's key 'transformations':\n",
        "\n",
        "- jax.jit\n",
        "- jax.vmap\n",
        "- jax.grad\n",
        "\n",
        "These transformations take a function as an argument and return a transformed version of the function. \n",
        "\n",
        "You can use the transformations as functions or decorators. For example:\n",
        "\n",
        "```python3\n",
        "\n",
        "def cos_sin(x):\n",
        "    return jnp.cos(jnp.sin(x))\n",
        "\n",
        "jitted_cos_sin = jax.jit(cos_sin)\n",
        "\n",
        "```\n",
        "\n",
        "or\n",
        "\n",
        "```python3\n",
        "@jax.jit\n",
        "def jitted_cos_sin(x):\n",
        "    return jnp.cos(jnp.sin(x))\n",
        "```\n",
        "\n",
        "\n",
        "The magic behind transformations is the notion of a JAX Tracer. Tracers are abstract stand-ins for array objects, and are passed to JAX functions in order to extract the sequence of operations that the function encodes. The need for these tracers to be abstract also restricts the kind of functions we can write as we shall see in the following.\n",
        "\n",
        "We won't aim to understand what exactly is happening at the low level but more review the functionality and how to use it for our use case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_DEt7vdbpir"
      },
      "source": [
        "# Just-in-time (JIT) compilation\n",
        "\n",
        "As the name suggests, JIT compiles a function at run time, just in time for its execution. As mentioned above, JAX uses the Open XLA compiler ecosystem which natively supports accelerators.\n",
        "\n",
        "Lets try out jax.jit in a simple example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5PzxHRRbmAc"
      },
      "outputs": [],
      "source": [
        "def cosine_similatiries(x, y):\n",
        "  \"\"\"\n",
        "  Computes the cosine similarity between two vectors.\n",
        "\n",
        "  We will use very large input vectors for x and y\n",
        "  \"\"\"\n",
        "  x_norm = jnp.sum(x ** 2) ** 0.5\n",
        "  y_norm = jnp.sum(y ** 2) ** 0.5\n",
        "  return jnp.sum(x * y) / (x_norm * y_norm)\n",
        "\n",
        "def pythonic_cosine_similatiries(x, y):\n",
        "  \"\"\"\n",
        "  For comparison\n",
        "  \"\"\"\n",
        "  x = list(x)\n",
        "  y = list(y)\n",
        "  x_norm = 0.0\n",
        "  y_norm = 0.0\n",
        "  for i in range(len(x)):\n",
        "    x_norm += x[i] ** 2\n",
        "    y_norm += y[i] ** 2\n",
        "  x_norm = x_norm ** 0.5\n",
        "  y_norm = y_norm ** 0.5\n",
        "  return sum(x[i] * y[i] for i in range(len(x))) / (x_norm * y_norm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFoLhdtHqeHo"
      },
      "source": [
        "To JIT compile code we simple call jax.jit on the function which returns the jitted function.\n",
        "\n",
        "**Note**: compiliation occurs whenever the shape of the inputs change or whenever a variable specified as static is changed in value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVovByxvqdk3"
      },
      "outputs": [],
      "source": [
        "jitted_cosine_similatiries = jax.jit(cosine_similatiries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcIqEKtzd2Mr"
      },
      "source": [
        "To test our functions, lets make some random numbers, we explicitly deal with the random number generator keys - a JAX quirk that you quickly get used to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WqnmvmidA98"
      },
      "outputs": [],
      "source": [
        "N = 10000\n",
        "\n",
        "seed = 1337\n",
        "key = jax.random.key(seed)\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "x = jax.random.normal(subkey, (N,))\n",
        "key, subkey = jax.random.split(key)\n",
        "y = jax.random.normal(subkey, (N,))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KGa2KJmd2lL"
      },
      "source": [
        "We can compare the run times of our various implementations, python, JAX with and without JIT (as well as see how long the code takes to compile)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeKKwn3fdBzT"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "python_time = timeit.timeit(lambda: pythonic_cosine_similatiries(x, y), number=1)\n",
        "jax_time = timeit.timeit(lambda: cosine_similatiries(x, y), number=100)/100\n",
        "compile_time = timeit.timeit(lambda: jitted_cosine_similatiries(x, y), number=1)\n",
        "jit_time = timeit.timeit(lambda: jitted_cosine_similatiries(x, y), number=100)/100\n",
        "\n",
        "print(f'Python time: {1e3*python_time:.4f} ms')\n",
        "print(f'JAXPython time: {1e3*jax_time:.4f} ms')\n",
        "print(f'Compile time: {1e3*compile_time:.4f} ms')\n",
        "print(f'JIT time: {1e3*jit_time:.4f} ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENkJUqjTd3ms"
      },
      "source": [
        "JIT seems pretty amazing - it also can compile code for accelerators like GPU and CPU\n",
        "\n",
        "**However**, there are things you can't jit, including:\n",
        "\n",
        "- Functions with 'side effects' or impure functions, basically you need to pass all arguments to the function rather than relying on state\n",
        "- Functions which include dynamically shaped arrays, if any array in the function would change shape based on the inputs this makes the function not traceable\n",
        "- Functions which include python control flow (if/then/else), again this makes the function not traceable as you can't work out which branch of the if/then/else you should take at compile time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxEq9AhKd6FU"
      },
      "outputs": [],
      "source": [
        "def variable_length_result(x,n):\n",
        "  res = jnp.linspace(0.0,x,n)\n",
        "  return res\n",
        "\n",
        "x = 0.25\n",
        "n = 5\n",
        "\n",
        "nojit_res = variable_length_result(x,n)\n",
        "print(nojit_res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1PkGM_YkiGm"
      },
      "outputs": [],
      "source": [
        "jitted_res = jax.jit(variable_length_result)(x,n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-JzOJz3mNV9"
      },
      "outputs": [],
      "source": [
        "# static_argnames tells JAX to recompile on changes at these argument positions:\n",
        "jitted_res = jax.jit(variable_length_result, static_argnames=['n'])(x,n)\n",
        "print(jitted_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M3ewfjmmeJ9"
      },
      "source": [
        "The above also applies for python-like if statements, in general these should be avoid and conditionals should be done with in-built JAX conditionals such as jnp.where.\n",
        "\n",
        "Below is some example code that shows this issue and the solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2Pde2ramrSm"
      },
      "outputs": [],
      "source": [
        "def jax_w_nojittable_conditional(x, threshold):\n",
        "    if x > threshold:\n",
        "        return x\n",
        "    else:\n",
        "        return 0.0\n",
        "    \n",
        "# This will fail because the conditional depends on a JAX array value at tracing time\n",
        "failing_jit = jax.jit(jax_w_nojittable_conditional)\n",
        "try:\n",
        "    y = failing_jit(0.75, 0.5)\n",
        "    print(y)\n",
        "except Exception as e:\n",
        "    print(f'JIT failed with error: {e}')\n",
        "\n",
        "# This will work because we use jnp.where which is JIT-compatible (and vectorised)\n",
        "x = jnp.linspace(0.0,1.0,100)\n",
        "y = jnp.where(x > 0.5, x, 0)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3OHHRdgt-Fn"
      },
      "source": [
        "# Vectorising map (jax.vmap)\n",
        "\n",
        "Another key JAX functionality is the *vmap*, which allows you vectorise a function over (all or a subset) of its inputs. Basically, if you are performing the same operation along an axis of an array, vmap is made to handle this.\n",
        "\n",
        "This happens a lot with batches in an ML context but also fully parallel operations in scientific computing. A simple example here is batched dot-products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dot_products(a, b):\n",
        "    return jnp.dot(a, b)\n",
        "\n",
        "# Now we can vmap this function over the first axis of a and b\n",
        "batched_dot_products = jax.vmap(dot_products, in_axes=(0,0))\n",
        "# Create some batched data\n",
        "key, subkey = jax.random.split(key)\n",
        "a = jax.random.normal(subkey, (100, 50))\n",
        "key, subkey = jax.random.split(key)\n",
        "b = jax.random.normal(subkey, (100, 50))\n",
        "# Compute batched dot products\n",
        "result = batched_dot_products(a, b)\n",
        "print(result.shape) #(100,) - one dot product per batch item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For a more complex example, lets only vmap over some axes, but not others.\n",
        "\n",
        "Let us compute the mean and variance over the first axis of a 2D data array *x*, including a weighting term *w* which is the same for all rows of *x*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSu4esJ9uj9X"
      },
      "outputs": [],
      "source": [
        "def compute_cumulants(x,w):\n",
        "  mu = jnp.sum(w*x)/jnp.sum(w)\n",
        "  var = jnp.sum(w*(x-mu)**2)/jnp.sum(w)\n",
        "  return mu, var\n",
        "\n",
        "# in_axes specifies over which axis the vectorising map should occur\n",
        "# None here says no vectorisation should be used, i.e. use the same w for all x\n",
        "vmapped_compute_cumulants = jax.vmap(compute_cumulants,in_axes=(1,None))\n",
        "\n",
        "Nx1 = 100\n",
        "Nx2 = 20\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "x = jax.random.normal(subkey, (Nx1,Nx2))\n",
        "key, subkey = jax.random.split(key)\n",
        "w = jax.random.normal(subkey, (Nx1))**2\n",
        "\n",
        "mu, var = vmapped_compute_cumulants(x,w)\n",
        "print(mu.shape)\n",
        "print(var.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63OXKkK_laZU"
      },
      "source": [
        "# JAX quirks\n",
        "\n",
        "JAX's capabilities make for a few quirks when compared to native python/numpy code.\n",
        "\n",
        "## Random numbers\n",
        "\n",
        "We have already seen this in the random number generation above, JAX makes you keep explicit track of your Pseudo Random Number Generator (PRNG or just RNG) key. You can split the key to form more keys sharings the same root, such that it is deterministic "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "key = jax.random.key(seed=42)\n",
        "\n",
        "# Split the key to get new keys\n",
        "key1, key2 = jax.random.split(key)\n",
        "print(f'Original key: {key}, key1: {key1}, key2: {key2}')\n",
        "\n",
        "# We can use these keys to generate random numbers\n",
        "rand1 = jax.random.normal(key1, (5,))\n",
        "rand2 = jax.random.normal(key2, (5,))\n",
        "print(f'Random numbers from key1: {rand1}')\n",
        "print(f'Random numbers from key2: {rand2}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In-place updates\n",
        "\n",
        "We must interact with JAX arrays in a different way than in python/numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zco1UOKYm1QG"
      },
      "outputs": [],
      "source": [
        "x = jnp.array([1.0,2.0,3.0,4.0])\n",
        "\n",
        "# This is not allowed\n",
        "try:\n",
        "  x[1] = -2.0\n",
        "except TypeError as e:\n",
        "  print('Numpy-like array updates not allowed:')\n",
        "  print(e)\n",
        "\n",
        "# You have to use the following syntax to perform in-place update\n",
        "x = x.at[1].set(-2.0)\n",
        "print('\\nUsing in-place updates:')\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTrees\n",
        "\n",
        "\"A pytree is a container-like structure built out of container-like Python objects — “leaf” pytrees and/or more pytrees. A pytree can include lists, tuples, and dicts. A leaf is anything that’s not a pytree, such as an array, but a single leaf is also a pytree.\n",
        "\n",
        "In the context of machine learning, a pytree can contain:\n",
        "\n",
        " - Model parameters\n",
        " - Dataset entries\n",
        " - Reinforcement learning agent observations\n",
        "\n",
        "When working with datasets, you can often come across pytrees (such as lists of lists of dicts).\"\n",
        "\n",
        "JAX transformations work with Pytrees, as well as additional functionality from jax.tree.\n",
        "\n",
        "Easiest example is a dictionary, taking our mean and variance vmap example from before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pytree_compute_cumulants(inputs):\n",
        "  mu = jnp.sum(inputs['w']*inputs['x'])/jnp.sum(inputs['w'])\n",
        "  var = jnp.sum(inputs['w']*(inputs['x']-mu)**2)/jnp.sum(inputs['w'])\n",
        "  return dict(mu=mu, var=var)\n",
        "\n",
        "# in_axes specifies over which axis the vectorising map should occur\n",
        "# None here says no vectorisation should be used, i.e. use the same w for all x\n",
        "vmapped_pytree_compute_cumulants = jax.vmap(pytree_compute_cumulants,in_axes=({'x':1,'w':None},))\n",
        "\n",
        "Nx1 = 100\n",
        "Nx2 = 20\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "x = jax.random.normal(subkey, (Nx1,Nx2))\n",
        "key, subkey = jax.random.split(key)\n",
        "w = jax.random.normal(subkey, (Nx1))**2\n",
        "\n",
        "outputs = vmapped_pytree_compute_cumulants(dict(x=x,w=w))\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-obmONUbqCi"
      },
      "source": [
        "# Automatic Differentiation\n",
        "\n",
        "## jax.grad\n",
        "\n",
        "jax.grad is the simpliest interface with JAX's automatic differentation (AD) capability. It works exclusively on scalar output functions (input can be multi-dimensional) - and it implements *reverse-mode* AD as this is most efficient for many-to-one functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsIY-RNzboxC"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sin_func(x):\n",
        "  return jnp.sin(x)\n",
        "\n",
        "x = jnp.linspace(-5,5,100)\n",
        "\n",
        "# Note we need to vmap our gradded function as jax.grad operates on scalar output functions only\n",
        "grad_sin_func = jax.vmap(jax.grad(sin_func))\n",
        "y = grad_sin_func(x)\n",
        "\n",
        "plt.title(r'No surprises here, $\\frac{d}{dx} \\sin (x) = \\cos (x)$')\n",
        "plt.plot(x,y,'r',lw=2)\n",
        "plt.plot(x,jnp.cos(x),'k--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6_iSLyWbhHq"
      },
      "source": [
        "Or with Pytrees\n",
        "\n",
        "See the following example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhAziAr7bghc"
      },
      "outputs": [],
      "source": [
        "def sin_variable_w_func(input):\n",
        "  return jnp.sin(input['w']*input['x'])\n",
        "\n",
        "# First for scalars\n",
        "input = {'w' : 2.0, 'x' : 1.0}\n",
        "\n",
        "scalar_grad_sin_variable_w_func = jax.grad(sin_variable_w_func)\n",
        "scalar_grad_sin_variable_w_func(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compose transformations, here a vmap on a grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruSNILAXcS9V"
      },
      "outputs": [],
      "source": [
        "# Now for scalar w and vector x\n",
        "\n",
        "input = {'w' : 2.0, 'x' : jnp.linspace(-5,5,100)}\n",
        "\n",
        "vector_x_grad_sin_variable_w_func = jax.vmap(scalar_grad_sin_variable_w_func,\n",
        "                                             in_axes=({'w' : None, 'x' : 0},))\n",
        "out = vector_x_grad_sin_variable_w_func(input)\n",
        "\n",
        "plt.title(r'No surprises here, $\\frac{d}{dw} \\sin (w x) = x \\cos (w x)$')\n",
        "plt.plot(x,out['w'],'r')\n",
        "plt.plot(x,input['x']*jnp.cos(input['w']*input['x']),'k--')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMb6m-zSb1Zq"
      },
      "source": [
        "## jax.jacfwd and jax.jacrev\n",
        "\n",
        "We can also compute the gradients of functions with multi-dimensional outputs, in particular we can compute the Jacobian of function $f(x)$ where $x$ is n-dimensional and $f(x)$ returns an m-dimensional vector.\n",
        "\n",
        "Mathematically, $J_f(x)$ is the Jacobian:\n",
        "\n",
        "$$\n",
        "  J_f(x) = \\begin{bmatrix} \\frac{d f_1}{dx_1} & \\dots & \\frac{d f_1}{dx_n} \\\\\n",
        "  \\vdots & & \\vdots \\\\\n",
        "  \\frac{d f_m}{dx_1} & \\dots & \\frac{d f_m}{dx_n} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "We can compute the Jacobian in forward- or reverse-mode automatic differentiation (jacfwd and jacrev respectively)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHGUEU4Jb4ZD"
      },
      "outputs": [],
      "source": [
        "def sin_cos_func(x):\n",
        "  \"\"\"\n",
        "  Test function with two outputs\n",
        "  \"\"\"\n",
        "  return jnp.array([jnp.sum(jnp.sin(x)), jnp.sum(jnp.cos(x))])\n",
        "\n",
        "x = jnp.linspace(-5,5,100)\n",
        "\n",
        "forwardmode_jac = jax.jacfwd(sin_cos_func)\n",
        "reversemode_jac = jax.jacrev(sin_cos_func)\n",
        "\n",
        "forwardmode_out = forwardmode_jac(x)\n",
        "reversemode_out = reversemode_jac(x)\n",
        "\n",
        "print(f'The shape of the forward-mode Jacobian is {forwardmode_out.shape}')\n",
        "print(f'The shape of the reverse-mode Jacobian is {reversemode_out.shape}')\n",
        "\n",
        "assert jnp.isclose(forwardmode_out,reversemode_out).all()\n",
        "# Note forwardmode_out =/= reversemode_out ! they involve different FLOPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1qmHfYkfQe_"
      },
      "source": [
        "We noted in the lecture that reverse-mode is more efficient for many-to-few functions, lets test this with our many-to-few test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXc1HpC0fQs5"
      },
      "outputs": [],
      "source": [
        "forwardmode_jac = jax.jit(forwardmode_jac)\n",
        "reversemode_jac = jax.jit(reversemode_jac)\n",
        "\n",
        "x_big = jnp.linspace(-5,5,1000)\n",
        "\n",
        "forwardmode_time = timeit.timeit(lambda: forwardmode_jac(x_big), number=100)/100\n",
        "reversemode_time = timeit.timeit(lambda: reversemode_jac(x_big), number=100)/100\n",
        "\n",
        "print(f'Forward-mode time: {1e3*forwardmode_time:.4f} ms')\n",
        "print(f'Reverse-mode time: {1e3*reversemode_time:.4f} ms')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Classes and JAX\n",
        "\n",
        "Python classes and JAX are more difficult to mesh together as the 'state' of classes can mess with the tracing of a class method - see below for example from JAX docs.\n",
        "\n",
        "We implement a simple counter class which has an attribute n which is incremented. This counts as a side-effect so is not compatible with jit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Counter:\n",
        "  \"\"\"A simple counter.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = 0\n",
        "\n",
        "  def count(self) -> int:\n",
        "    \"\"\"Increments the counter and returns the new value.\"\"\"\n",
        "    self.n += 1\n",
        "    return self.n\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Resets the counter to zero.\"\"\"\n",
        "    self.n = 0\n",
        "\n",
        "\n",
        "counter = Counter()\n",
        "fast_counter = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "  print(f'fast_counter returns: {fast_counter()}')\n",
        "  print(f'counter.count() returns: {counter.count()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Equinox is JAX library which allows for class-like objects in JAX, this is useful for implementing neural networks. The parameters of the model can be attributes of an equinox module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import equinox as eqx\n",
        "\n",
        "class LinearLayer(eqx.Module):\n",
        "    # Define the parameters of the layer\n",
        "    # This looks similar to a PyTorch nn.Module or python dataclass\n",
        "    w: jnp.ndarray\n",
        "    b: jnp.ndarray\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, key: jnp.ndarray):\n",
        "        w_key, b_key = jax.random.split(key)\n",
        "        self.w = jax.random.normal(w_key, (out_features, in_features))\n",
        "        self.b = jax.random.normal(b_key, (out_features,))\n",
        "\n",
        "    def __call__(self,x) -> jnp.ndarray:\n",
        "        return jnp.dot(self.w, x) + self.b\n",
        "    \n",
        "key, subkey = jax.random.split(key)\n",
        "layer = LinearLayer(in_features=3, out_features=2, key=subkey)\n",
        "x = jnp.array([1.0, 2.0, 3.0])\n",
        "y = layer(x)\n",
        "print(y)\n",
        "print(f'Layer parameters: w shape {layer.w.shape}, b shape {layer.b.shape}')   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimisation in JAX - optax\n",
        "\n",
        "Another useful JAX library is optax. It implements key optimisers in JAX - allowing the training of ML models (or any gradient-based optimisation problem).\n",
        "\n",
        "Here is an example we will see again in exercise 2, a simple convex optimisation problem using gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "def example_loss(p):\n",
        "  return jnp.sum(p**2)\n",
        "\n",
        "# Initialize parameters of the \"model\" + optimizer.\n",
        "learning_rate = 1e-1\n",
        "optimizer = optax.sgd(learning_rate)\n",
        "\n",
        "p0 = jnp.ones((10,))\n",
        "opt_state = optimizer.init(p0)\n",
        "\n",
        "# A simple update loop\n",
        "Nepoch = 300\n",
        "grad_loss = jax.value_and_grad(example_loss)\n",
        "p = p0.copy()\n",
        "history = []\n",
        "for _ in range(Nepoch):\n",
        "  loss,grads = grad_loss(p)\n",
        "  # Optax optimizer uses the gradients to update the parameters and the optimiser state\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  p = optax.apply_updates(p, updates)\n",
        "  history.append(loss)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.semilogy(history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmLUSusRb_kr"
      },
      "source": [
        "## Continuing AD\n",
        "\n",
        "### **Advanced**: Jacobian-vector products (JVPs) and vector-Jacobian products (VJPs)\n",
        "\n",
        "To really understand what is happening under the hood, we need to introduce JVPs and VJPs, which are related to forward- and reverse-mode AD respectively.\n",
        "\n",
        "Let’s say you have a function that takes several inputs and returns several outputs. Think of this like a physical system where you tweak some parameters (the inputs) and observe some measurements (the outputs). The Jacobian of this function is like a giant matrix of sensitivities—how each output changes with each input.\n",
        "\n",
        "But in practice, we don’t usually need the whole matrix when applying the chain rule.\n",
        "\n",
        "Instead, we want to know what happens in a particular direction. That’s where Jacobian-vector products (JVPs) and vector-Jacobian products (VJPs) come in.\n",
        "\n",
        "**Jacobian-Vector Product (JVP)**\n",
        "\n",
        "This is used in forward-mode AD. It answers the question:\n",
        "\n",
        "*“If I nudge my inputs a little bit in some direction, how do the outputs change?”*\n",
        "\n",
        "Imagine you’re computing a function, $f(x)$, and you have some small perturbation $\\delta x$. The JVP tells you how that perturbation propagates through the function:\n",
        "\n",
        "$$\n",
        "JVP =  J_f(x) \\cdot \\delta x\n",
        "$$\n",
        "\n",
        "You can think of the JVP like sending a small ripple through your system from the input side and seeing how it affects the output.\n",
        "\n",
        "The JVP is efficient when the function, $f(x)$, is few-to-many (small number of inputs, large number of outputs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkKPE28D4Y6r"
      },
      "outputs": [],
      "source": [
        "def test_function(x):\n",
        "  res = {\n",
        "      'f0' : (jnp.sin(x[1])+x[2]*jnp.sin(x[3]))*jnp.exp(x[0]),\n",
        "      'f1' : (jnp.sin(x[1])+x[2]*jnp.sin(x[3]))*jnp.exp(-x[0])\n",
        "  }\n",
        "  return res\n",
        "\n",
        "x = jnp.array([1.0,2.0,3.0,4.0])\n",
        "\n",
        "# To get df/dx[i] for each i, we need to sweep through the JVP on the input space\n",
        "for i in range(len(x)):\n",
        "  v = jnp.eye(len(x))[i]\n",
        "  y, u = jax.jvp(test_function, (x,), (v,))\n",
        "  print(f'df/dx[{i}] = {u}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ooz0HBb4ZEC"
      },
      "source": [
        "**Vector-Jacobian Product (VJP)**\n",
        "\n",
        "This is used in reverse-mode AD. It answers the question:\n",
        "\n",
        "*“How does a particular change in the outputs trace back to affect the inputs?”*\n",
        "\n",
        "$$\n",
        "VJP = \\delta y^T \\cdot J_f(x)\n",
        "$$\n",
        "\n",
        "This is like backpropagating a signal from the output side—asking how a change in a measurement pushes back through the system to influence the inputs. This is the basis for how gradients are computed efficiently when you have lots of inputs but only one output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG8MKWx6zF2A"
      },
      "outputs": [],
      "source": [
        "y, vjp_fun = jax.vjp(test_function, x)\n",
        "\n",
        "# To get df[j]/dx for each j, we need to sweep through the VJP on the output space\n",
        "v = {'f0' : 1.0, 'f1' : 0.0}\n",
        "u = vjp_fun(v)\n",
        "print(f'df[0]/dx = {u}')\n",
        "\n",
        "v = {'f0' : 0.0, 'f1' : 1.0}\n",
        "u = vjp_fun(v)\n",
        "print(f'df[1]/dx = {u}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euu5lGDTqAd1"
      },
      "source": [
        "Note we get the same answer as with the JVP but with two calls to the VJP vs four calls to the VJP.\n",
        "\n",
        "As a sanity check, here we compute the Jacobian using jacfwd and jacrev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yRMSguMpAm8"
      },
      "outputs": [],
      "source": [
        "print(jax.jacfwd(test_function)(x))\n",
        "print(jax.jacrev(test_function)(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdWqHQtVqMtL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMumXWmRYEPXSyMj0bqNxqc",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lagradept_update",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
